function [thetas,errors]=descentpoly(tf,gtf,theta0,X,Y,lr,varargin)  normalizer_type="normal";#normalizer_type="minmax";## Normalize the datanx = normalizer(normalizer_type);Xo = nx.fit_transform(X);#Xo=X;## The outputs vector with the original datany = normalizer(normalizer_type);Yo = ny.fit_transform(Y);  %% Parse all given parameters  order = length(theta0)-1;  if (order<1)    error("Initial point theta0 must have at least 2 dimensions");  elseif (order>20)    error("Currently order limit set to 20");  endif  ## theta0 must be a row vector  if (isvector(theta0))    theta0=theta0(:).'; # esta linea convierte cualquier vector en vector fila  else    error("theta0 must be a row vector");  endif      ## Xo must be a column vector  if (isvector(Xo))    Xo=Xo(:);  else    error("Xo must be a column vector");  endif  ## Yo must be a column vector  if (isvector(Yo))    Yo=Yo(:);  else    error("Yo must be a column vector");  endif    defaultMethod="batch";  defaultBeta=0.9;  defaultBeta2=0.99;  defaultMaxIter=200;  defaultEps=0.001;  defaultMinibatch=1;  defaultMaxerror=10000;  p = inputParser;  validMethods={"batch","stochastic","momentum","rmsprop","adam"};  checkMethod = @(x) any(validatestring(x,validMethods));  addParameter(p,'method',defaultMethod,checkMethod);  checkBeta = @(x) isreal(x) && isscalar(x) && x>=0 && x<=1;  checkRealPosScalar = @(x) isreal(x) && isscalar(x) && x>0;  addParameter(p,'beta',defaultBeta,checkBeta);  addParameter(p,'beta2',defaultBeta2,checkBeta);  addParameter(p,'maxiter',defaultMaxIter,checkRealPosScalar);  addParameter(p,'epsilon',defaultEps,checkRealPosScalar);  addParameter(p,'minibatch',defaultMinibatch,checkRealPosScalar);  addParameter(p,'maxerror',defaultMaxerror,checkRealPosScalar);    parse(p,varargin{:});    if ~checkBeta(lr)    error("Learning rate must be between 0 and 1");  endif    method = p.Results.method;       ## String with desired method  beta = p.Results.beta;           ## Momentum parameters beta  beta2 = p.Results.beta2;         ## ADAM paramter beta2  maxiter = p.Results.maxiter;     ## maxinum number of iterations  epsilon = p.Results.epsilon;     ## convergence error tolerance  minibatch = p.Results.minibatch; ## minibatch size  maxerror = p.Results.maxerror;    #thetas = [theta0];  #errors = [tf(theta0,Xo,Yo)];ts=theta0;rmspepsilon=1e-8;  if(strcmp(method,"batch"))    for i=[1:maxiter] # max iteraciones    tc = ts(end,:); # Current position, se va a la ultima fila de la matriz ts    gn = gtf(tc,Xo,Yo);  # Gradient at current position    tn = tc - lr * gn;# Next position    ts = [ts;tn];# agrego la nueva posicion al final de la matriz    # ts es una matriz con todos los betas qe se usaron para aprender    error1=loss(tn,Xo,Yo);      if ((norm(gn)<epsilon)) break; endif; #parametro para detener antes       if (maxerror<error1)         break; endif;
    endfor  elseif (strcmp(method,"stochastic"));   j=0;   for i=[1:maxiter]     tc = ts(end,:); # Current position    sample=round(unifrnd(1,rows(Xo),minibatch,1)); # Use MB random samples    gn = gtf(tc,Xo(sample,:),Yo(sample));  # Gradient at current position    tn = tc - lr * gn;# Next position    ts = [ts;tn];    error1=loss(tn,Xo,Yo);    if ((norm(tc-tn)<epsilon))      j=j+1;      if (j>5) ## Only exit if several times the positions have been close enough        break;      endif;    else      j=0;    endif;    if (maxerror<error1) break; endif;  endfor      elseif (strcmp(method,"momentum"))   sample=round(unifrnd(1,rows(Xo),minibatch,1)); # Use MB random samples for init.   V = gtf(ts,Xo(sample,:),Yo(sample));  # Gradiente para inicializar   j=0;   for i=[1:maxiter]     tc = ts(end,:); # Current position    sample=round(unifrnd(1,rows(Xo),minibatch,1)); # Use MB random samples    gn = gtf(tc,Xo(sample,:),Yo(sample));  # Gradient at current position    V = beta*V + (1-beta)*gn; ## Filter the gradient    tn = tc - lr* V;      ## Gradient descent with filtered grad    ts = [ts;tn];    error1=loss(tn,Xo,Yo);    if(norm(tc-tn)<epsilon)      j=j+1;      if (j>5) ## Only exit if several times the positions have been close enough        break;      endif;    else      j=0;    endif;    if (maxerror<error1) break; endif;   endfor    elseif (strcmp(method,"rmsprop"));   rmspepsilon=1e-8;   sample=round(unifrnd(1,rows(Xo),minibatch,1)); # Use MB random samples for init.   gn = gtf(ts,Xo(sample,:),Yo(sample));  # Gradient at current position   s = gn.^2;   j=0;   for i=[1:maxiter] # max iterations    tc = ts(end,:); # Current position    sample=round(unifrnd(1,rows(Xo),minibatch,1)); # Use MB random samples    gn = gtf(tc,Xo(sample,:),Yo(sample));  # Gradient at current position    s = beta2*s + (1-beta2)*(gn.^2);    gg = gn./(sqrt(s + rmspepsilon) );    tn = tc - lr * gg;      ## Gradient descent with filtered grad    ts = [ts;tn];    error1=loss(tn,Xo,Yo);    if (norm(tc-tn)<epsilon)      j=j+1;      if (j>2) ## Only exit if several times the positions have been close enough        break;      endif;    else      j=0;    endif;    if (maxerror<error1) break; endif;   endfor  elseif (strcmp(method,"adam"))     sample=round(unifrnd(1,rows(Xo),minibatch,1)); # Use MB random samples for init.   gn = gtf(ts,Xo(sample,:),Yo(sample))  # Gradient at current position   s = gn.^2; # Initialize to avoid bias   V = gn;   j=0;   #mgradientes=gn;#para control   for i=[1:maxiter] # max 500 iterations    tc = ts(end,:); # Current position    sample=round(unifrnd(1,rows(Xo),minibatch,1)); # Use MB random samples    gn = gtf(tc,Xo(sample,:),Yo(sample));  # Gradient at current position     #mgradientes=[mgradientes;gn];    s = beta2*s + (1-beta2)*(gn.^2);    V = beta*V + (1-beta)*gn;    gg = V./(sqrt(s + rmspepsilon) );    tn = tc - lr * gg;      ## Gradient descent with filtered grad    ts = [ts;tn];    error1=loss(tn,Xo,Yo);    if (norm(tc-tn)<epsilon)      j=j+1;      if (j>5) ## Only exit if several times the positions have been close enough        break;      endif;    else      j=0;    endif;    if (maxerror<error1) break; endif;   endfor  else   prinf("error en method ");   ###a este punto todos los métodos están implementados  endif    thetas=ts;errors=tf(thetas,Xo,Yo);endfunction